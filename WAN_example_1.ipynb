{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WAN_example_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "73JOInUUqCp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICES_ORDER\"]= \"PCI_BUS_IS\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
        "\n",
        "class pde_wan():\n",
        "    \n",
        "    def __init__(self, dim=2):\n",
        "        import numpy as np\n",
        "        global np\n",
        "        #\n",
        "        import time\n",
        "        global time\n",
        "        #\n",
        "        #import tensorflow as tf\n",
        "        import tensorflow.compat.v1 as tf\n",
        "        global tf\n",
        "        tf.disable_v2_behavior()\n",
        "        #\n",
        "        import matplotlib.pyplot as plt\n",
        "        global plt\n",
        "        #\n",
        "        from scipy.interpolate import griddata\n",
        "        global griddata\n",
        "        #\n",
        "        self.up= 1.0\n",
        "        self.low=-1.0\n",
        "        \n",
        "        self.mesh_size= 256\n",
        "        #\n",
        "        self.v_layer= 6\n",
        "        self.v_hidden_size= 50\n",
        "        self.v_step= 1       \n",
        "        self.v_rate=0.015   \n",
        "        self.u_layer= 6\n",
        "        self.u_hidden_size= 20\n",
        "        self.u_step= 1\n",
        "        self.u_rate=0.015\n",
        "        self.batch_size= 10000 \n",
        "        self.test_size=  5000\n",
        "        self.bound_size= 100#(*2*dim)\n",
        "        self.iteration=  20001 \n",
        "        self.dim= dim \n",
        "        self.dir= './problem_weak/'\n",
        "    def sample_train(self, dm_size, bd_size, dim):      ##dm-domain;bd-boundry;dim-dimension\n",
        "        low= self.low; up= self.up;\n",
        "        #*********************************************************\n",
        "        # collocation points in domain\n",
        "        x_dm= np.random.uniform(low, up, [dm_size, dim])     ##up=1;low=-1;dim=2\n",
        "        #*********************************************************\n",
        "        # collocation points on boundary\n",
        "        x_bd_list=[]  #x_boundry\n",
        "        for i in range(dim):\n",
        "            x_bound= np.random.uniform(low, up, [bd_size, dim])\n",
        "            x_bound[:,i]= up\n",
        "            x_bd_list.append(x_bound)\n",
        "            x_bound= np.random.uniform(low, up, [bd_size, dim])\n",
        "            x_bound[:,i]= low\n",
        "            x_bd_list.append(x_bound)\n",
        "        x_bd= np.concatenate(x_bd_list, axis=0)\n",
        "        #*********************************************************\n",
        "        int_dm= (up-low)**dim        \n",
        "        #********************************************************\n",
        "        x= np.reshape(x_dm[:,0], [-1,1])\n",
        "        f_dm= np.where(x<=0, 2, -2)\n",
        "        #*********************************************************\n",
        "        # u(x) on boundary\n",
        "        x= np.reshape(x_bd[:,0], [-1,1])\n",
        "        u_bd= np.where(x<=0,-x**2, x**2)\n",
        "        #*********************************************************\n",
        "        x_dm= np.float32(x_dm)  \n",
        "        x_bd= np.float32(x_bd)\n",
        "        int_dm= np.float32(int_dm)\n",
        "        f_dm= np.float32(f_dm)\n",
        "        u_bd= np.float32(u_bd)\n",
        "        return(x_dm, f_dm, x_bd, u_bd, int_dm)      ##int_dm=integrand domain\n",
        "\n",
        "\n",
        "    def sample_test(self, test_size, dim):\n",
        "        up= self.up; low= self.low;\n",
        "        #**********************************************************\n",
        "        x_mesh= np.linspace(low, up, self.mesh_size)\n",
        "        mesh= np.meshgrid(x_mesh, x_mesh)\n",
        "        x= np.reshape(mesh[0], [-1,1])\n",
        "        y= np.reshape(mesh[1], [-1,1])\n",
        "        x_dm= np.concatenate((x,y), 1) \n",
        "        #***********************************************************\n",
        "        u_dm= np.where(x<=0, -x**2, x**2)     ##x应该是x_dm吗？？？  ##u_dm的形式和论文中不一样\n",
        "        #***********************************************************\n",
        "        x_dm= np.float32(x_dm)\n",
        "        u_dm= np.float32(u_dm)\n",
        "        return(mesh, x_dm, u_dm)\n",
        "\n",
        "\n",
        "    def DNN_u(self, x_in, out_size, name, reuse):\n",
        "        h_size= self.u_hidden_size\n",
        "        with tf.variable_scope(name, reuse= reuse):   ##可以让变量有相同命名\n",
        "            hi= tf.layers.dense(x_in, h_size, activation= tf.nn.tanh,    ##tf.layers.dense表示添加一个层，全连接层\n",
        "                                name='input_layer')\n",
        "            hi= tf.layers.dense(hi, h_size, activation= tf.nn.tanh,\n",
        "                                name='input_layer1')\n",
        "            for i in range(self.u_layer):\n",
        "                if i%2==0:\n",
        "                    hi= tf.layers.dense(hi, h_size, activation= tf.nn.softplus,\n",
        "                                        name='h_layer'+str(i))\n",
        "                else:\n",
        "                    hi= tf.sin(tf.layers.dense(hi, h_size), name='h_layer'+str(i))\n",
        "            out= tf.layers.dense(hi, out_size, name='out_layer')\n",
        "        return(out)\n",
        "\n",
        "\n",
        "    def DNN_v(self, x_in, out_size, name, reuse):\n",
        "        h_size= self.v_hidden_size\n",
        "        with tf.variable_scope(name, reuse= reuse):\n",
        "            hi= tf.layers.dense(x_in, h_size, activation= tf.nn.tanh,\n",
        "                                name='input_layer')\n",
        "            hi= tf.layers.dense(hi, h_size, activation= tf.nn.softplus,\n",
        "                                name='input_layer1')\n",
        "            for i in range(self.v_layer):\n",
        "                if i%2==0:\n",
        "                    hi= tf.layers.dense(hi, h_size, activation= tf.nn.softplus,\n",
        "                                                            name='h_layer'+str(i))\n",
        "                else:\n",
        "                    hi= tf.sin(tf.layers.dense(hi, h_size), name='h_layer'+str(i))\n",
        "            out= tf.layers.dense(hi, out_size, name='out_layer')\n",
        "        return(out)\n",
        "\n",
        "\n",
        "    def grad_u(self, x, out_size, name):\n",
        "        #**************************************\n",
        "        # u(x,y)\n",
        "        fun_u= self.DNN_u(x, out_size, name, tf.AUTO_REUSE)   \n",
        "        #*************************************\n",
        "        # grad_u(x,y)\n",
        "        grad_u= tf.gradients(fun_u, x, unconnected_gradients='zero')[0]\n",
        "        return(fun_u, grad_u)\n",
        "\n",
        "\n",
        "    def grad_v(self, x, out_size, name):\n",
        "        #**************************************\n",
        "        # v(x,y)\n",
        "        fun_v= self.DNN_v(x, out_size, name, tf.AUTO_REUSE)\n",
        "        #*************************************\n",
        "        # grad_v(x,y)\n",
        "        grad_v= tf.gradients(fun_v, x, unconnected_gradients='zero')[0]\n",
        "        return(fun_v, grad_v)\n",
        "\n",
        "\n",
        "    def fun_a(self, x):   ##PDE公式中的系数a\n",
        "        #********************************************************\n",
        "        out= 1.0\n",
        "        return(out)\n",
        "\n",
        "\n",
        "    def fun_w(self, x, low=-1.0, up=1.0):    ##phi函数的一部分，phi=w*v，其中w在boundary为0\n",
        "        I1= 0.210987\n",
        "        x_list= tf.split(x, self.dim, 1)\n",
        "        #**************************************************\n",
        "        x_scale_list=[]\n",
        "        h_len= (up-low)/2.0\n",
        "        for i in range(self.dim):\n",
        "          x_scale= (x_list[i]-low-h_len)/h_len   ##做标准化处理\n",
        "          x_scale_list.append(x_scale)\n",
        "        #*************************************************\n",
        "        z_x_list=[];\n",
        "        for i in range(self.dim):\n",
        "          supp_x= tf.greater(1-tf.abs(x_scale_list[i]), 0)\n",
        "          z_x= tf.where(supp_x, tf.exp(1/(tf.pow(x_scale_list[i], 2)-1))/I1, \n",
        "                     tf.zeros_like(x_scale_list[i]))  \n",
        "          z_x_list.append(z_x)\n",
        "        #***************************************************\n",
        "        w_val= tf.constant(1.0)  ##创建常量\n",
        "        for i in range(self.dim):\n",
        "          w_val= tf.multiply(w_val, z_x_list[i])\n",
        "          dw= tf.gradients(w_val, x, unconnected_gradients='zero')[0]\n",
        "          dw= tf.where(tf.is_nan(dw), tf.zeros_like(dw), dw)\n",
        "        return(w_val, dw)\n",
        "\n",
        "\n",
        "    def build(self):\n",
        "        #**************************************************************\n",
        "        with tf.name_scope('placeholder'):\n",
        "            self.x_domain= tf.placeholder(tf.float32, shape=[None, self.dim], name='x_dm')\n",
        "            self.f_obv= tf.placeholder(tf.float32, shape=[None, 1], name='f_obv')\n",
        "            self.x_bound= tf.placeholder(tf.float32, shape=[None, self.dim], name='x_b')\n",
        "            self.g_obv= tf.placeholder(tf.float32, shape=[None, 1], name='g_obv')\n",
        "            self.int_domain= tf.placeholder(tf.float32, shape=(), name='int_domain')\n",
        "        #**************************************************************\n",
        "        name_u= 'dnn_u'; name_v= 'dnn_v';\n",
        "        self.u_val, self.du= self.grad_u(self.x_domain, 1, name_u)\n",
        "        self.v_val, self.dv= self.grad_v(self.x_domain, 1, name_v)\n",
        "        self.w_val, self.dw= self.fun_w(self.x_domain)\n",
        "        u_bound= self.DNN_u(self.x_bound, 1, name_u, tf.AUTO_REUSE)\n",
        "        #\n",
        "        a_val= self.fun_a(self.x_domain)\n",
        "        self.wv= tf.multiply(self.w_val, self.v_val)\n",
        "        #\n",
        "        du_dw= tf.reduce_sum(tf.multiply(self.du, self.dw), axis=1)  \n",
        "        du_dw= tf.reshape(du_dw, [-1,1])\n",
        "        #\n",
        "        du_dv= tf.reduce_sum(tf.multiply(self.du, self.dv), axis=1)\n",
        "        du_dv= tf.reshape(du_dv, [-1,1])\n",
        "        #\n",
        "        du_dwv= tf.add(tf.multiply(self.v_val, du_dw),   ##分部计算dwv，du相当于已知\n",
        "                       tf.multiply(self.w_val, du_dv))\n",
        "        #**************************************************************\n",
        "        with tf.name_scope('loss'):\n",
        "            # \n",
        "            v_int= tf.multiply(tf.reduce_mean(self.v_val**2), self.int_domain)  ##phi的2范数的平方  ##为什么这里没有w？？？\n",
        "            #\n",
        "            with tf.name_scope('u_loss'):\n",
        "                #\n",
        "                int_l1= tf.multiply(tf.reduce_mean(tf.multiply(\n",
        "                    a_val,du_dwv)), self.int_domain)\n",
        "                int_r= tf.multiply(tf.reduce_mean(\n",
        "                    tf.multiply(self.f_obv, self.wv)), self.int_domain)  \n",
        "                #\n",
        "                int_l1_w= tf.multiply(tf.reduce_mean(tf.multiply(      ##loss_int_w只算了w没有v，意义是什么？？？\n",
        "                    a_val,du_dw)), self.int_domain)\n",
        "                int_r_w= tf.multiply(tf.reduce_mean(\n",
        "                    tf.multiply(self.f_obv, self.w_val)), self.int_domain)\n",
        "                #\n",
        "                self.loss_int= tf.square(int_l1-int_r) / v_int\n",
        "                self.loss_int_w= tf.square(int_l1_w-int_r_w)                        \n",
        "                # \n",
        "                self.loss_bound= tf.reduce_sum(tf.abs(u_bound-self.g_obv))   \n",
        "                #\n",
        "                self.loss_u= (50)*self.loss_bound+(1.0)*self.loss_int#+(0.0)*self.loss_int_w      ##为什么loss_int_w系数是0？？？\n",
        "            with tf.name_scope('v_loss'):\n",
        "                self.loss_v=  - tf.log(self.loss_int) \n",
        "        #**************************************************************\n",
        "        # \n",
        "        u_vars= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dnn_u')\n",
        "        v_vars= tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='dnn_v')\n",
        "        #***************************************************************\n",
        "        # \n",
        "        with tf.name_scope('optimizer'):\n",
        "            self.u_opt= tf.train.AdagradOptimizer(self.u_rate).minimize(       ##优化\n",
        "                    self.loss_u, var_list= u_vars)\n",
        "            self.v_opt= tf.train.AdagradOptimizer(self.v_rate).minimize(\n",
        "                    self.loss_v, var_list= v_vars)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        #****************************************************************\n",
        "        tf.reset_default_graph(); self.build(); \n",
        "        #***************************************************************\n",
        "        mesh, test_x, test_u= self.sample_test(self.test_size, self.dim);    ##sample test函数的输出\n",
        "        step=[]; error_l2r=[]; error_l2=[]; \n",
        "        time_begin=time.time(); time_list=[]; iter_time_list=[]\n",
        "        #***************************************************************\n",
        "        #saver= tf.train.Saver()\n",
        "        with tf.Session() as sess:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            #****************************\n",
        "            for i in range(self.iteration):\n",
        "                # mini-batch data\n",
        "                train_data= self.sample_train(self.batch_size, self.bound_size, self.dim)  #batch_size是什么意思？？\n",
        "                feed_train={self.x_domain: train_data[0],    ##sample_train函数的输出\n",
        "                            self.f_obv: train_data[1],\n",
        "                            self.x_bound: train_data[2],\n",
        "                            self.g_obv: train_data[3],\n",
        "                            self.int_domain: train_data[4]}\n",
        "                if i%5==0:\n",
        "                    #*********************************\n",
        "                    pred_u, pred_v= sess.run([self.u_val, self.v_val],feed_dict={self.x_domain: test_x})   ##用test_x得到新的u_val,v_val输出\n",
        "                    err_l2= np.sqrt(np.mean(np.square(test_u-pred_u)))   ##pred_u是按神经网络方法计算得到的u\n",
        "                    u_norm= np.sqrt(np.mean(np.square(test_u)))   ##u的范数吗？？？\n",
        "                    step.append(i+1); \n",
        "                    error_l2r.append(err_l2/u_norm); error_l2.append(err_l2)  ##error_12r比error_12多除了一个u_norm\n",
        "                    time_step= time.time(); time_list.append(time_step-time_begin)            \n",
        "                if i%200==0:\n",
        "                    loss_v, loss_u, int_u, intw_u, loss_bd= sess.run(\n",
        "                        [self.loss_v, self.loss_u, self.loss_int, self.loss_int_w, self.loss_bound], \n",
        "                        feed_dict= feed_train)\n",
        "                    print('Iterations:{}'.format(i))\n",
        "                    print('u_loss:{} v_loss:{}'.format(loss_u, loss_v))\n",
        "                    print('int_u:{} int_u_w:{} loss_bd:{} err_l2r:{}'.format(\n",
        "                        int_u, intw_u, loss_bd, error_l2r[-1]))  ##[-1]取最后一个元素\n",
        "                    #self.show_error(step, error_l2r, self.dim, 'l2r')  ##show error什么意思？？？\n",
        "                    #self.show_v_val(mesh, test_x, pred_v, 'v', i)\n",
        "                    #self.show_u_val(mesh, test_x, test_u, pred_u, 'u', i)\n",
        "                #\n",
        "                iter_time0= time.time()\n",
        "                for _ in range(self.v_step):  ##v_step步长\n",
        "                    _ = sess.run(self.v_opt, feed_dict=feed_train)\n",
        "                for _ in range(self.u_step):\n",
        "                    _ = sess.run(self.u_opt, feed_dict=feed_train)\n",
        "                iter_time_list.append(time.time()-iter_time0)\n",
        "                #\n",
        "            #*******************************************\n",
        "            #self.show_error_abs(mesh, test_x, np.abs(test_u-pred_u), self.dim)\n",
        "            #self.show_u_val(mesh, test_x, test_u, pred_u, 'u', i)\n",
        "            print('L2_e is {}, L2_re is {}'.format(np.min(error_l2), np.min(error_l2r)))\n",
        "            print('Running time is:{}'.format(time_list[-1]))\n",
        "        return(mesh, test_x, test_u, pred_u, step, error_l2, error_l2r, time_list, iter_time_list, self.dim)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH9KrFY9ntLb",
        "colab_type": "code",
        "outputId": "4d2a76d7-abde-4e6f-aa2e-b11a04ea2a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__=='__main__':\n",
        "    demo= pde_wan()\n",
        "    mesh, test_x, test_u, pred_u, step, error_l2, error_l2r, time_list, iter_time_list, dim = demo.train()\n",
        "    #***************************\n",
        "    # save data as .mat form\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import os\n",
        "    os.chdir('/content/drive/My Drive/Colab_Notebooks')\n",
        "    file_path= './problem_smooth/'   #文件路径为当前运行目录下的/problem_smooth/\n",
        "\n",
        "    import scipy.io\n",
        "    data_save= {}\n",
        "    data_save['mesh']= mesh  ##是一个list，里面有两个矩阵，矩阵的元素是将domain划分好的x取值点，\n",
        "    data_save['test_x']= test_x  ##在domain内x的取值点，由mesh而来\n",
        "    data_save['test_u']= test_u  ##由test_x得到的u值\n",
        "    data_save['pred_u']= pred_u  ##由test_x通过神经网络得到的u值\n",
        "    data_save['step']= step   ##每隔5次迭代输出的次数\n",
        "    data_save['error_l2']= error_l2   ##test_u,pred_u的差\n",
        "    data_save['error_l2r']= error_l2r   ##比error_12r多除一个u_norm\n",
        "    data_save['time_list']= time_list   ##每5次迭代的总运行时间\n",
        "    data_save['iter_time_list']= iter_time_list  ##每次迭代的总运行时间\n",
        "    scipy.io.savemat(file_path+'wan_pde_2d', data_save)\n",
        "    print('Data saved in '+file_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From <ipython-input-1-39bc6fba6be0>:98: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "Iterations:0\n",
            "u_loss:18663.486328125 v_loss:7.902535438537598\n",
            "int_u:0.0003698046784847975 int_u_w:0.003246146719902754 loss_bd:373.26971435546875 err_l2r:2.174706220626831\n",
            "Iterations:200\n",
            "u_loss:1626.0313720703125 v_loss:-3.982668876647949\n",
            "int_u:53.66004943847656 int_u_w:0.039001427590847015 loss_bd:31.447425842285156 err_l2r:0.5552060008049011\n",
            "Iterations:400\n",
            "u_loss:1107.57275390625 v_loss:-4.4889235496521\n",
            "int_u:89.02556610107422 int_u_w:0.13618212938308716 loss_bd:20.370943069458008 err_l2r:0.6825895309448242\n",
            "Iterations:600\n",
            "u_loss:708.6657104492188 v_loss:-4.545591831207275\n",
            "int_u:94.2161636352539 int_u_w:0.6791151165962219 loss_bd:12.288990020751953 err_l2r:0.6912499666213989\n",
            "Iterations:800\n",
            "u_loss:632.8995971679688 v_loss:-4.655168056488037\n",
            "int_u:105.12686920166016 int_u_w:0.678232729434967 loss_bd:10.55545425415039 err_l2r:0.6817077398300171\n",
            "Iterations:1000\n",
            "u_loss:646.1854248046875 v_loss:-4.59195613861084\n",
            "int_u:98.68730163574219 int_u_w:0.5732390880584717 loss_bd:10.949962615966797 err_l2r:0.6813868284225464\n",
            "Iterations:1200\n",
            "u_loss:568.0635375976562 v_loss:-4.537386417388916\n",
            "int_u:93.44625091552734 int_u_w:0.46296802163124084 loss_bd:9.492345809936523 err_l2r:0.6640753149986267\n",
            "Iterations:1400\n",
            "u_loss:515.0160522460938 v_loss:-4.528879642486572\n",
            "int_u:92.65471649169922 int_u_w:0.5376044511795044 loss_bd:8.447226524353027 err_l2r:0.6604418754577637\n",
            "Iterations:1600\n",
            "u_loss:513.1209716796875 v_loss:-4.887438774108887\n",
            "int_u:132.61346435546875 int_u_w:0.463243693113327 loss_bd:7.610149383544922 err_l2r:0.6470743417739868\n",
            "Iterations:1800\n",
            "u_loss:484.58154296875 v_loss:-4.812997817993164\n",
            "int_u:123.10010528564453 int_u_w:0.4250314235687256 loss_bd:7.229629039764404 err_l2r:0.5983127951622009\n",
            "Iterations:2000\n",
            "u_loss:458.2648620605469 v_loss:-4.824604034423828\n",
            "int_u:124.53716278076172 int_u_w:0.9673390984535217 loss_bd:6.674553871154785 err_l2r:0.5817877650260925\n",
            "Iterations:2200\n",
            "u_loss:392.9612731933594 v_loss:-4.761545181274414\n",
            "int_u:116.92645263671875 int_u_w:0.5115153193473816 loss_bd:5.52069616317749 err_l2r:0.550593912601471\n",
            "Iterations:2400\n",
            "u_loss:433.5586853027344 v_loss:-4.557128429412842\n",
            "int_u:95.30939483642578 int_u_w:0.9930433034896851 loss_bd:6.764986038208008 err_l2r:0.5394876003265381\n",
            "Iterations:2600\n",
            "u_loss:450.7131042480469 v_loss:-4.574456214904785\n",
            "int_u:96.97531127929688 int_u_w:0.7270417809486389 loss_bd:7.074755668640137 err_l2r:0.5205221772193909\n",
            "Iterations:2800\n",
            "u_loss:415.2584228515625 v_loss:-4.392045497894287\n",
            "int_u:80.8055419921875 int_u_w:0.7148640751838684 loss_bd:6.68905782699585 err_l2r:0.5147017240524292\n",
            "Iterations:3000\n",
            "u_loss:352.8344421386719 v_loss:-4.245240211486816\n",
            "int_u:69.77252960205078 int_u_w:0.7077904343605042 loss_bd:5.661238193511963 err_l2r:0.4906218945980072\n",
            "Iterations:3200\n",
            "u_loss:349.5812072753906 v_loss:-4.004886150360107\n",
            "int_u:54.86556625366211 int_u_w:0.2637743055820465 loss_bd:5.894312858581543 err_l2r:0.4747142493724823\n",
            "Iterations:3400\n",
            "u_loss:400.5146789550781 v_loss:-4.138838768005371\n",
            "int_u:62.72993850708008 int_u_w:0.812645673751831 loss_bd:6.75569486618042 err_l2r:0.46662619709968567\n",
            "Iterations:3600\n",
            "u_loss:393.90887451171875 v_loss:-3.910604953765869\n",
            "int_u:49.92915344238281 int_u_w:0.23604542016983032 loss_bd:6.879593849182129 err_l2r:0.45102351903915405\n",
            "Iterations:3800\n",
            "u_loss:316.2463073730469 v_loss:-3.950232744216919\n",
            "int_u:51.94745635986328 int_u_w:0.5925586223602295 loss_bd:5.285976886749268 err_l2r:0.4439152479171753\n",
            "Iterations:4000\n",
            "u_loss:309.0200500488281 v_loss:-3.7061779499053955\n",
            "int_u:40.697959899902344 int_u_w:0.14656977355480194 loss_bd:5.36644172668457 err_l2r:0.41726574301719666\n",
            "Iterations:4200\n",
            "u_loss:318.7422180175781 v_loss:-3.6686503887176514\n",
            "int_u:39.198970794677734 int_u_w:0.19527482986450195 loss_bd:5.590865135192871 err_l2r:0.40848857164382935\n",
            "Iterations:4400\n",
            "u_loss:315.6556091308594 v_loss:-3.6962833404541016\n",
            "int_u:40.2972526550293 int_u_w:0.29331743717193604 loss_bd:5.507167339324951 err_l2r:0.4084940552711487\n",
            "Iterations:4600\n",
            "u_loss:320.97113037109375 v_loss:-3.622727394104004\n",
            "int_u:37.439537048339844 int_u_w:0.31674766540527344 loss_bd:5.6706318855285645 err_l2r:0.41058841347694397\n",
            "Iterations:4800\n",
            "u_loss:263.77703857421875 v_loss:-3.480358123779297\n",
            "int_u:32.471351623535156 int_u_w:0.23759199678897858 loss_bd:4.6261138916015625 err_l2r:0.3847801685333252\n",
            "Iterations:5000\n",
            "u_loss:271.41412353515625 v_loss:-3.4458134174346924\n",
            "int_u:31.36878776550293 int_u_w:0.36028605699539185 loss_bd:4.800907135009766 err_l2r:0.37652677297592163\n",
            "Iterations:5200\n",
            "u_loss:317.02032470703125 v_loss:-3.320563316345215\n",
            "int_u:27.67593765258789 int_u_w:0.23409514129161835 loss_bd:5.7868876457214355 err_l2r:0.3828035295009613\n",
            "Iterations:5400\n",
            "u_loss:251.69686889648438 v_loss:-3.359924793243408\n",
            "int_u:28.78702735900879 int_u_w:0.32160481810569763 loss_bd:4.458196640014648 err_l2r:0.37137308716773987\n",
            "Iterations:5600\n",
            "u_loss:251.98049926757812 v_loss:-3.3697779178619385\n",
            "int_u:29.072072982788086 int_u_w:0.07617995887994766 loss_bd:4.4581685066223145 err_l2r:0.366181343793869\n",
            "Iterations:5800\n",
            "u_loss:322.3262023925781 v_loss:-3.5138721466064453\n",
            "int_u:33.57803726196289 int_u_w:0.16083022952079773 loss_bd:5.77496337890625 err_l2r:0.36737415194511414\n",
            "Iterations:6000\n",
            "u_loss:272.2720947265625 v_loss:-3.2487549781799316\n",
            "int_u:25.75824737548828 int_u_w:0.04748842865228653 loss_bd:4.930276870727539 err_l2r:0.3551732003688812\n",
            "Iterations:6200\n",
            "u_loss:252.43634033203125 v_loss:-3.392063617706299\n",
            "int_u:29.727235794067383 int_u_w:0.05707705765962601 loss_bd:4.454182147979736 err_l2r:0.34976598620414734\n",
            "Iterations:6400\n",
            "u_loss:252.7665252685547 v_loss:-3.1731386184692383\n",
            "int_u:23.882322311401367 int_u_w:0.08855197578668594 loss_bd:4.577683925628662 err_l2r:0.33891794085502625\n",
            "Iterations:6600\n",
            "u_loss:272.3074035644531 v_loss:-3.1201589107513428\n",
            "int_u:22.64997673034668 int_u_w:0.06084824725985527 loss_bd:4.9931488037109375 err_l2r:0.33620503544807434\n",
            "Iterations:6800\n",
            "u_loss:221.97291564941406 v_loss:-3.289039373397827\n",
            "int_u:26.81709098815918 int_u_w:0.1406935453414917 loss_bd:3.903116464614868 err_l2r:0.32504963874816895\n",
            "Iterations:7000\n",
            "u_loss:217.53463745117188 v_loss:-3.0216243267059326\n",
            "int_u:20.52460289001465 int_u_w:0.051467034965753555 loss_bd:3.9402008056640625 err_l2r:0.3203486204147339\n",
            "Iterations:7200\n",
            "u_loss:249.19561767578125 v_loss:-2.970041513442993\n",
            "int_u:19.49273109436035 int_u_w:0.06065979227423668 loss_bd:4.594057559967041 err_l2r:0.31045791506767273\n",
            "Iterations:7400\n",
            "u_loss:256.0613098144531 v_loss:-3.0641865730285645\n",
            "int_u:21.41703224182129 int_u_w:0.001383499358780682 loss_bd:4.692885398864746 err_l2r:0.3038598895072937\n",
            "Iterations:7600\n",
            "u_loss:234.8461151123047 v_loss:-3.0251858234405518\n",
            "int_u:20.59783363342285 int_u_w:0.03358275070786476 loss_bd:4.284965515136719 err_l2r:0.31366318464279175\n",
            "Iterations:7800\n",
            "u_loss:242.48899841308594 v_loss:-2.884256362915039\n",
            "int_u:17.890256881713867 int_u_w:0.147744283080101 loss_bd:4.491974830627441 err_l2r:0.3091273307800293\n",
            "Iterations:8000\n",
            "u_loss:202.73016357421875 v_loss:-2.9097201824188232\n",
            "int_u:18.351661682128906 int_u_w:0.07182208448648453 loss_bd:3.687570333480835 err_l2r:0.29839783906936646\n",
            "Iterations:8200\n",
            "u_loss:234.9862823486328 v_loss:-2.99519944190979\n",
            "int_u:19.989347457885742 int_u_w:0.014571542851626873 loss_bd:4.299938678741455 err_l2r:0.2825718820095062\n",
            "Iterations:8400\n",
            "u_loss:236.41583251953125 v_loss:-3.0414865016937256\n",
            "int_u:20.936342239379883 int_u_w:0.031091837212443352 loss_bd:4.309589862823486 err_l2r:0.2920924425125122\n",
            "Iterations:8600\n",
            "u_loss:226.94639587402344 v_loss:-2.982577323913574\n",
            "int_u:19.738622665405273 int_u_w:0.00018528132932260633 loss_bd:4.144155502319336 err_l2r:0.2947697341442108\n",
            "Iterations:8800\n",
            "u_loss:239.0941162109375 v_loss:-3.0074830055236816\n",
            "int_u:20.236400604248047 int_u_w:0.23353888094425201 loss_bd:4.377154350280762 err_l2r:0.2958941161632538\n",
            "Iterations:9000\n",
            "u_loss:210.4499053955078 v_loss:-3.001814603805542\n",
            "int_u:20.122018814086914 int_u_w:0.17765434086322784 loss_bd:3.8065576553344727 err_l2r:0.2808934450149536\n",
            "Iterations:9200\n",
            "u_loss:228.5214385986328 v_loss:-3.0198307037353516\n",
            "int_u:20.487821578979492 int_u_w:0.12510228157043457 loss_bd:4.160672187805176 err_l2r:0.2862752377986908\n",
            "Iterations:9400\n",
            "u_loss:221.97439575195312 v_loss:-2.500312566757202\n",
            "int_u:12.186301231384277 int_u_w:0.07848789542913437 loss_bd:4.1957621574401855 err_l2r:0.27847525477409363\n",
            "Iterations:9600\n",
            "u_loss:223.37881469726562 v_loss:-2.7851662635803223\n",
            "int_u:16.202512741088867 int_u_w:0.07253063470125198 loss_bd:4.143526077270508 err_l2r:0.2762453854084015\n",
            "Iterations:9800\n",
            "u_loss:243.83950805664062 v_loss:-2.4754531383514404\n",
            "int_u:11.887091636657715 int_u_w:0.11990247666835785 loss_bd:4.6390485763549805 err_l2r:0.2788383960723877\n",
            "Iterations:10000\n",
            "u_loss:221.4043426513672 v_loss:-2.783947706222534\n",
            "int_u:16.18277931213379 int_u_w:0.07629600167274475 loss_bd:4.10443115234375 err_l2r:0.28054940700531006\n",
            "Iterations:10200\n",
            "u_loss:281.90704345703125 v_loss:-2.663618803024292\n",
            "int_u:14.34811782836914 int_u_w:0.0838063508272171 loss_bd:5.3511786460876465 err_l2r:0.26991137862205505\n",
            "Iterations:10400\n",
            "u_loss:280.6362609863281 v_loss:-2.6765024662017822\n",
            "int_u:14.534171104431152 int_u_w:0.07200352102518082 loss_bd:5.3220415115356445 err_l2r:0.27264267206192017\n",
            "Iterations:10600\n",
            "u_loss:193.41476440429688 v_loss:-2.5460221767425537\n",
            "int_u:12.75625991821289 int_u_w:0.12241525202989578 loss_bd:3.6131701469421387 err_l2r:0.2665753662586212\n",
            "Iterations:10800\n",
            "u_loss:249.37156677246094 v_loss:-2.7925758361816406\n",
            "int_u:16.323009490966797 int_u_w:0.1537153124809265 loss_bd:4.660971164703369 err_l2r:0.2646492123603821\n",
            "Iterations:11000\n",
            "u_loss:209.5750732421875 v_loss:-2.868704319000244\n",
            "int_u:17.614179611206055 int_u_w:0.06326325982809067 loss_bd:3.8392179012298584 err_l2r:0.2506791055202484\n",
            "Iterations:11200\n",
            "u_loss:231.4171600341797 v_loss:-2.840996265411377\n",
            "int_u:17.132823944091797 int_u_w:0.17410485446453094 loss_bd:4.285686492919922 err_l2r:0.27203917503356934\n",
            "Iterations:11400\n",
            "u_loss:240.0342254638672 v_loss:-2.6704463958740234\n",
            "int_u:14.446416854858398 int_u_w:0.0729033499956131 loss_bd:4.511756420135498 err_l2r:0.261125385761261\n",
            "Iterations:11600\n",
            "u_loss:271.6180419921875 v_loss:-2.96921706199646\n",
            "int_u:19.476665496826172 int_u_w:0.11074388027191162 loss_bd:5.042827606201172 err_l2r:0.258564829826355\n",
            "Iterations:11800\n",
            "u_loss:198.46957397460938 v_loss:-2.8288519382476807\n",
            "int_u:16.926015853881836 int_u_w:0.15220177173614502 loss_bd:3.630871295928955 err_l2r:0.240263432264328\n",
            "Iterations:12000\n",
            "u_loss:225.50497436523438 v_loss:-2.718515157699585\n",
            "int_u:15.157798767089844 int_u_w:0.062321923673152924 loss_bd:4.206943511962891 err_l2r:0.2639071047306061\n",
            "Iterations:12200\n",
            "u_loss:183.92623901367188 v_loss:-2.490576982498169\n",
            "int_u:12.0682373046875 int_u_w:0.06282831728458405 loss_bd:3.437160015106201 err_l2r:0.25474533438682556\n",
            "Iterations:12400\n",
            "u_loss:222.09153747558594 v_loss:-2.8225111961364746\n",
            "int_u:16.819034576416016 int_u_w:0.02760809101164341 loss_bd:4.10545015335083 err_l2r:0.24914877116680145\n",
            "Iterations:12600\n",
            "u_loss:189.58810424804688 v_loss:-2.596302032470703\n",
            "int_u:13.414042472839355 int_u_w:0.19288252294063568 loss_bd:3.5234811305999756 err_l2r:0.25144559144973755\n",
            "Iterations:12800\n",
            "u_loss:194.3620147705078 v_loss:-2.4397802352905273\n",
            "int_u:11.470519065856934 int_u_w:0.022042548283934593 loss_bd:3.657829761505127 err_l2r:0.2412048727273941\n",
            "Iterations:13000\n",
            "u_loss:224.17086791992188 v_loss:-2.4892477989196777\n",
            "int_u:12.052206993103027 int_u_w:0.008845950476825237 loss_bd:4.242373466491699 err_l2r:0.24042752385139465\n",
            "Iterations:13200\n",
            "u_loss:221.85508728027344 v_loss:-2.723029375076294\n",
            "int_u:15.226380348205566 int_u_w:0.12283950299024582 loss_bd:4.132574081420898 err_l2r:0.24640275537967682\n",
            "Iterations:13400\n",
            "u_loss:218.7759552001953 v_loss:-2.534862518310547\n",
            "int_u:12.614696502685547 int_u_w:0.16590261459350586 loss_bd:4.123225212097168 err_l2r:0.24510519206523895\n",
            "Iterations:13600\n",
            "u_loss:198.93382263183594 v_loss:-2.6327898502349854\n",
            "int_u:13.912530899047852 int_u_w:0.062452755868434906 loss_bd:3.700425624847412 err_l2r:0.24937871098518372\n",
            "Iterations:13800\n",
            "u_loss:193.89625549316406 v_loss:-2.85245943069458\n",
            "int_u:17.330350875854492 int_u_w:0.0076873404905200005 loss_bd:3.531318187713623 err_l2r:0.2374345064163208\n",
            "Iterations:14000\n",
            "u_loss:226.00302124023438 v_loss:-2.560737133026123\n",
            "int_u:12.945356369018555 int_u_w:0.162245512008667 loss_bd:4.261153221130371 err_l2r:0.2545630931854248\n",
            "Iterations:14200\n",
            "u_loss:203.7910614013672 v_loss:-2.5512776374816895\n",
            "int_u:12.82347583770752 int_u_w:0.13828174769878387 loss_bd:3.8193519115448 err_l2r:0.23362663388252258\n",
            "Iterations:14400\n",
            "u_loss:191.54150390625 v_loss:-2.762510061264038\n",
            "int_u:15.839552879333496 int_u_w:0.20725782215595245 loss_bd:3.5140390396118164 err_l2r:0.24478556215763092\n",
            "Iterations:14600\n",
            "u_loss:168.83746337890625 v_loss:-2.4217631816864014\n",
            "int_u:11.265705108642578 int_u_w:0.0020489422604441643 loss_bd:3.151435375213623 err_l2r:0.23409877717494965\n",
            "Iterations:14800\n",
            "u_loss:231.96311950683594 v_loss:-2.8999781608581543\n",
            "int_u:18.173748016357422 int_u_w:0.00771325035020709 loss_bd:4.275787353515625 err_l2r:0.2361893355846405\n",
            "Iterations:15000\n",
            "u_loss:234.59530639648438 v_loss:-2.6259024143218994\n",
            "int_u:13.817035675048828 int_u_w:0.14655421674251556 loss_bd:4.415565490722656 err_l2r:0.23554451763629913\n",
            "Iterations:15200\n",
            "u_loss:183.99160766601562 v_loss:-2.5834462642669678\n",
            "int_u:13.242696762084961 int_u_w:0.04841362312436104 loss_bd:3.414978265762329 err_l2r:0.23477387428283691\n",
            "Iterations:15400\n",
            "u_loss:187.984375 v_loss:-2.467454671859741\n",
            "int_u:11.792391777038574 int_u_w:0.008413382805883884 loss_bd:3.5238397121429443 err_l2r:0.22962318360805511\n",
            "Iterations:15600\n",
            "u_loss:232.66065979003906 v_loss:-2.7857398986816406\n",
            "int_u:16.211807250976562 int_u_w:0.2601187825202942 loss_bd:4.328977108001709 err_l2r:0.22890318930149078\n",
            "Iterations:15800\n",
            "u_loss:202.5329132080078 v_loss:-2.5615158081054688\n",
            "int_u:12.955439567565918 int_u_w:0.43025314807891846 loss_bd:3.7915494441986084 err_l2r:0.23116254806518555\n",
            "Iterations:16000\n",
            "u_loss:211.24908447265625 v_loss:-2.3465969562530518\n",
            "int_u:10.44994831085205 int_u_w:0.22325852513313293 loss_bd:4.015982627868652 err_l2r:0.2264668047428131\n",
            "Iterations:16200\n",
            "u_loss:224.97523498535156 v_loss:-2.7346296310424805\n",
            "int_u:15.404035568237305 int_u_w:0.007435797248035669 loss_bd:4.1914238929748535 err_l2r:0.22503215074539185\n",
            "Iterations:16400\n",
            "u_loss:162.41958618164062 v_loss:-2.59507417678833\n",
            "int_u:13.397581100463867 int_u_w:0.007173287682235241 loss_bd:2.980440139770508 err_l2r:0.2257585972547531\n",
            "Iterations:16600\n",
            "u_loss:151.8889923095703 v_loss:-2.201047897338867\n",
            "int_u:9.034475326538086 int_u_w:0.008071607910096645 loss_bd:2.857090473175049 err_l2r:0.22557386755943298\n",
            "Iterations:16800\n",
            "u_loss:142.919921875 v_loss:-2.4434144496917725\n",
            "int_u:11.512280464172363 int_u_w:0.048986177891492844 loss_bd:2.628152847290039 err_l2r:0.2206784039735794\n",
            "Iterations:17000\n",
            "u_loss:181.4334259033203 v_loss:-2.863771438598633\n",
            "int_u:17.527507781982422 int_u_w:0.0010724588064476848 loss_bd:3.278118371963501 err_l2r:0.2164415568113327\n",
            "Iterations:17200\n",
            "u_loss:166.89596557617188 v_loss:-2.4651172161102295\n",
            "int_u:11.764860153198242 int_u_w:0.09435635805130005 loss_bd:3.1026220321655273 err_l2r:0.2140025794506073\n",
            "Iterations:17400\n",
            "u_loss:214.8160858154297 v_loss:-2.011638641357422\n",
            "int_u:7.475557804107666 int_u_w:0.10195903480052948 loss_bd:4.146810531616211 err_l2r:0.22056038677692413\n",
            "Iterations:17600\n",
            "u_loss:204.81118774414062 v_loss:-2.7593162059783936\n",
            "int_u:15.789044380187988 int_u_w:0.12142527848482132 loss_bd:3.780442714691162 err_l2r:0.2303587645292282\n",
            "Iterations:17800\n",
            "u_loss:181.5701904296875 v_loss:-2.753654718399048\n",
            "int_u:15.699906349182129 int_u_w:0.060472652316093445 loss_bd:3.3174057006835938 err_l2r:0.2178414762020111\n",
            "Iterations:18000\n",
            "u_loss:198.97372436523438 v_loss:-1.7532715797424316\n",
            "int_u:5.773460388183594 int_u_w:0.12787194550037384 loss_bd:3.8640053272247314 err_l2r:0.21438564360141754\n",
            "Iterations:18200\n",
            "u_loss:200.53012084960938 v_loss:-2.1326756477355957\n",
            "int_u:8.437411308288574 int_u_w:0.02294469252228737 loss_bd:3.8418540954589844 err_l2r:0.2200939804315567\n",
            "Iterations:18400\n",
            "u_loss:183.44236755371094 v_loss:-2.3328559398651123\n",
            "int_u:10.307336807250977 int_u_w:0.02632000856101513 loss_bd:3.462700605392456 err_l2r:0.2143157422542572\n",
            "Iterations:18600\n",
            "u_loss:178.96482849121094 v_loss:-2.804232120513916\n",
            "int_u:16.514389038085938 int_u_w:0.002871618140488863 loss_bd:3.2490086555480957 err_l2r:0.2010108381509781\n",
            "Iterations:18800\n",
            "u_loss:188.60992431640625 v_loss:-2.5846495628356934\n",
            "int_u:13.258642196655273 int_u_w:0.0002640218008309603 loss_bd:3.507025718688965 err_l2r:0.21717438101768494\n",
            "Iterations:19000\n",
            "u_loss:171.9824676513672 v_loss:-2.5273594856262207\n",
            "int_u:12.520401954650879 int_u_w:0.0014160319697111845 loss_bd:3.189241409301758 err_l2r:0.21528339385986328\n",
            "Iterations:19200\n",
            "u_loss:167.76255798339844 v_loss:-2.6684892177581787\n",
            "int_u:14.418170928955078 int_u_w:0.057692211121320724 loss_bd:3.066887855529785 err_l2r:0.21384622156620026\n",
            "Iterations:19400\n",
            "u_loss:190.8101348876953 v_loss:-2.542428493499756\n",
            "int_u:12.710501670837402 int_u_w:0.11233945190906525 loss_bd:3.561992645263672 err_l2r:0.20954136550426483\n",
            "Iterations:19600\n",
            "u_loss:180.94192504882812 v_loss:-2.531266689300537\n",
            "int_u:12.569416999816895 int_u_w:0.06602063030004501 loss_bd:3.36745023727417 err_l2r:0.20676907896995544\n",
            "Iterations:19800\n",
            "u_loss:192.2969970703125 v_loss:-2.0637052059173584\n",
            "int_u:7.875093936920166 int_u_w:0.006597074680030346 loss_bd:3.6884381771087646 err_l2r:0.2095230221748352\n",
            "Iterations:20000\n",
            "u_loss:145.8277587890625 v_loss:-2.0776281356811523\n",
            "int_u:7.9855055809021 int_u_w:0.10905663669109344 loss_bd:2.756844997406006 err_l2r:0.20835930109024048\n",
            "L2_e is 0.08891425281763077, L2_re is 0.1972731053829193\n",
            "Running time is:3839.5168266296387\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Data saved in ./problem_smooth/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}